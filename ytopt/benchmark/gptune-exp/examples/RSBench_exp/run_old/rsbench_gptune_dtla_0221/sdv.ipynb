{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sdv.tabular import GaussianCopula\n",
    "from sdv.tabular import CopulaGAN\n",
    "from sdv.evaluation import evaluate\n",
    "from sdv.tabular import CTGAN\n",
    "from sdv.tabular import TVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('xsbench_input/results_rf_sml_xsbench_speedup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(np.unique(df['input'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_10 = np.quantile(df.objective.values, 0.9)\n",
    "print(q_10)\n",
    "q_10_s = np.quantile(df.speedup.values, 0.9)\n",
    "print(q_10_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real_df = df.loc[df['objective'] < q_10]\n",
    "real_df = df.loc[df['speedup'] > q_10_s]\n",
    "real_data = real_df.drop(columns=['elapsed_sec'])\n",
    "real_data = real_data.drop(columns=['objective'])\n",
    "# real_data = real_data.drop(columns=['speedup'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.max(real_data['speedup'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(2.521757519763745e-51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(column_data):\n",
    "    return np.log2(column_data)\n",
    "def reverse_transform(column_data):\n",
    "    return np.power(2,column_data.round())\n",
    "\n",
    "def transform_input(column_data):\n",
    "    return column_data/100000.0 \n",
    "def reverse_transform_input(column_data):\n",
    "    return 100000.0*column_data.round()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.constraints import CustomConstraint, Between\n",
    "constraint = CustomConstraint(\n",
    "    columns=['p4','p5'],\n",
    "    transform=transform,\n",
    "    reverse_transform=reverse_transform\n",
    "    )\n",
    "\n",
    "constraint2 = CustomConstraint(\n",
    "    columns=['input'],\n",
    "    transform=transform_input,\n",
    "    reverse_transform=reverse_transform_input\n",
    "    )\n",
    "\n",
    "constraint_input = Between(\n",
    "    column='input',\n",
    "    low=100000,\n",
    "    high=10000001,\n",
    "    handling_strategy='transform'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = GaussianCopula(\n",
    "            field_names = ['input','p0','p1','p2','p3','p4','p5','p6','p7','speedup'],    \n",
    "            field_transformers = {'input': 'integer',\n",
    "                                  'p0': 'categorical',\n",
    "                                  'p1': 'categorical',\n",
    "                                  'p2': 'categorical',\n",
    "                                  'p3': 'categorical',\n",
    "                                  'p4': 'categorical',\n",
    "                                  'p5': 'categorical',\n",
    "                                  'p6': 'categorical', \n",
    "                                  'p7': 'categorical',\n",
    "                                  'speedup': 'float'},\n",
    "#             constraints=[constraint]\n",
    "            constraints=[constraint_input]\n",
    "    )\n",
    "model.fit(real_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ss = model.sample()\n",
    "evaluate(ss, real_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(ss['input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([ss,real_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10000000000 in np.unique(result['input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluate(ss, real_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "synthetic_data = model.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data_test = synthetic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data_test['input'] = 5000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "input_sizes['s']  = [100000] \n",
    "input_sizes['sm'] = [500000]\n",
    "input_sizes['m']  = [1000000]\n",
    "input_sizes['ml'] = [2500000]\n",
    "input_sizes['l']  = [5000000]\n",
    "input_sizes['xl'] = [10000000] \n",
    "'''\n",
    "input_sizes = {}\n",
    "input_sizes['s']  = [100000] \n",
    "input_sizes['sm'] = [500000]\n",
    "input_sizes['m']  = [1000000]\n",
    "input_sizes['ml'] = [2500000]\n",
    "input_sizes['l']  = [5000000]\n",
    "input_sizes['xl'] = [10000000]\n",
    "# model.fit(real_data)\n",
    "conditions = pd.DataFrame({'input': [input_sizes['sm'][0],input_sizes['ml'][0],input_sizes['xl'][0]], 'speedup':[7.0,7.0,7.0]})\n",
    "\n",
    "conditions = {'input': input_sizes['sm'][0]}#, 'speedup':7.7}\n",
    "\n",
    "# conditions = pd.DataFrame({'gender': ['M', 'M', 'M', 'F', 'F', 'F']})\n",
    "ss1 = model.sample(10,conditions=conditions)#,float_rtol=10.)#,float_rtol=10.)#, max_retries=10000)\n",
    "#                    max_retries=10000, \n",
    "#                    max_rows_multiplier=100,  \n",
    "#                    float_rtol=0.1,\n",
    "#                    graceful_reject_sampling=True)\n",
    "score = evaluate(ss1, real_data)\n",
    "print(score)\n",
    "ss1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[input_sizes['sm'][0],input_sizes['ml'][0],input_sizes['xl'][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = dict(ss1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in ss1.iterrows():\n",
    "    a = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row[1].values[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_point_val = row[1].values[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_point_val[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sizes['sm']*0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "---------------------------------------------------------------------------\n",
    "ValueError                                Traceback (most recent call last)\n",
    "/tmp/ipykernel_81158/1465504640.py in <module>\n",
    "      2 conditions = {'input': 10000000, 'speedup':7.0}\n",
    "      3 \n",
    "----> 4 ss1 = model.sample(num_rows=1, conditions=conditions)\n",
    "      5 #                    max_retries=10000,\n",
    "      6 #                    max_rows_multiplier=100,\n",
    "\n",
    "~/anaconda3/envs/ytune/lib/python3.7/site-packages/sdv/tabular/base.py in sample(self, num_rows, max_retries, max_rows_multiplier, conditions, float_rtol, graceful_reject_sampling)\n",
    "    500                         transformed_condition,\n",
    "    501                         float_rtol,\n",
    "--> 502                         graceful_reject_sampling\n",
    "    503                     )\n",
    "    504                     all_sampled_rows.append(sampled_rows)\n",
    "\n",
    "~/anaconda3/envs/ytune/lib/python3.7/site-packages/sdv/tabular/base.py in _conditionally_sample_rows(self, dataframe, max_retries, max_rows_multiplier, condition, transformed_condition, float_rtol, graceful_reject_sampling)\n",
    "    376             if len(sampled_rows) == 0:\n",
    "    377                 error = 'No valid rows could be generated with the given conditions.'\n",
    "--> 378                 raise ValueError(error)\n",
    "    379 \n",
    "    380             elif not graceful_reject_sampling:\n",
    "\n",
    "ValueError: No valid rows could be generated with the given conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data['input'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_distributions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.get_likelihood(synthetic_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.get_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(synthetic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "synthetic_data.to_csv('out.csv')#(index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sdv.tabular import GaussianCopula\n",
    "from sdv.tabular import CopulaGAN\n",
    "from sdv.evaluation import evaluate\n",
    "from sdv.tabular import CTGAN\n",
    "from sdv.tabular import TVAE\n",
    "\n",
    "#Plotting tools\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import numpy as np\n",
    "from numpy.random import multivariate_normal\n",
    "import matplotlib.ticker as mtick\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors.kde import KernelDensity\n",
    "# import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from autotune import TuningProblem\n",
    "from autotune.space import *\n",
    "import os, sys, time, json, math\n",
    "import ConfigSpace as CS\n",
    "import ConfigSpace.hyperparameters as CSH\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm\n",
    "import csv\n",
    "# from csv import writer\n",
    "# from csv import reader\n",
    "# from sklearn import preprocessing\n",
    "# HERE = os.path.dirname(os.path.abspath(__file__))\n",
    "# sys.path.insert(1, os.path.dirname(HERE)+ '/plopper')\n",
    "# from plopper import Plopper\n",
    "\n",
    "# cs = CS.ConfigurationSpace(seed=1234)\n",
    "# # number of threads\n",
    "# p0= CSH.OrdinalHyperparameter(name='p0', sequence=['2','3','4','5','6','7','8'], default_value='8')\n",
    "# #block size for openmp dynamic schedule\n",
    "# p1= CSH.OrdinalHyperparameter(name='p1', sequence=['10','20','40','64','80','100','128','160','200'], default_value='100')\n",
    "# #clang unrolling\n",
    "# p2= CSH.CategoricalHyperparameter(name='p2', choices=[\"#pragma clang loop unrolling full\", \" \"], default_value=' ')\n",
    "# #omp parallel\n",
    "# p3= CSH.CategoricalHyperparameter(name='p3', choices=[\"#pragma omp parallel for\", \" \"], default_value=' ')\n",
    "# # tile size for one dimension for 2D tiling\n",
    "# p4= CSH.OrdinalHyperparameter(name='p4', sequence=['2','4','8','16','32','64','96','128','256'], default_value='96')\n",
    "# # tile size for another dimension for 2D tiling\n",
    "# p5= CSH.OrdinalHyperparameter(name='p5', sequence=['2','4','8','16','32','64','96','128','256'], default_value='256')\n",
    "# # omp placement\n",
    "# p6= CSH.CategoricalHyperparameter(name='p6', choices=['cores','threads','sockets'], default_value='cores')\n",
    "# p7= CSH.CategoricalHyperparameter(name='p7', choices=['compact','scatter','balanced','none','disabled', 'explicit'], default_value='none')\n",
    "\n",
    "# cs.add_hyperparameters([p0, p1, p2, p3, p4, p5, p6, p7])\n",
    "\n",
    "# dir_path = os.path.dirname(os.path.realpath(__file__))\n",
    "# kernel_idx = dir_path.rfind('/')\n",
    "# kernel = dir_path[kernel_idx+1:]\n",
    "# obj = Plopper(dir_path+'/mmp.c',dir_path)\n",
    "\n",
    "# x1=['p0','p1','p2','p3','p4','p5','p6','p7']\n",
    "# exe_times = []\n",
    "# def myobj(point: dict):\n",
    "\n",
    "#   def plopper_func(x):\n",
    "#     x = np.asarray_chkfinite(x)  # ValueError if any NaN or Inf\n",
    "#     value = [point[x1[0]],point[x1[1]],point[x1[2]],point[x1[3]],point[x1[4]],point[x1[5]],point[x1[6]],point[x1[7]]] \n",
    "#     print('VALUES:',point[x1[0]])\n",
    "#     params = [\"P0\",\"P1\",\"P2\",\"P3\",\"P4\",\"P5\",\"P6\",\"P7\"]\n",
    "\n",
    "#     result = obj.findRuntime(value, params, ' -s large -m event -l 500000') # defined(MINI_DATASET) && !defined(SMALL_DATASET) && !defined(MEDIUM_DATASET) && !defined(LARGE_DATASET) && !defined(EXTRALARGE_DATASET) && !defined(HUGE_DATASET)\n",
    "#     return result\n",
    "\n",
    "#   x = np.array([point[f'p{i}'] for i in range(len(point))])  \n",
    "#   results = plopper_func(x)\n",
    "#   exe_times.append(results)\n",
    "#   np.save(dir_path+'/exe_times_sdv.npy',exe_times)\n",
    "# #   results_s = sorted(results)\n",
    "# #   results_m = results_s[1:-1]\n",
    "#   print('OUTPUT:%f',results, float(np.mean(results[1:])))\n",
    "#   return float(np.mean(results[1:]))\n",
    "\n",
    "df = pd.read_csv('./results_rf_sml_xsbench_speedup.csv')\n",
    "q_10 = np.quantile(df.objective.values, 0.9)\n",
    "print(q_10)\n",
    "q_10_s = np.quantile(df.speedup.values, 0.9)\n",
    "print(q_10_s)\n",
    "# real_df = df.loc[df['objective'] < q_10]\n",
    "real_df = df.loc[df['speedup'] > q_10_s]\n",
    "real_data = real_df.drop(columns=['elapsed_sec'])\n",
    "real_data = real_data.drop(columns=['objective'])\n",
    "# real_data = real_data.drop(columns=['speedup'])\n",
    "\n",
    "max_speedup = np.max(real_data['speedup'])\n",
    "\n",
    "def transform(column_data):\n",
    "    return np.log2(column_data)\n",
    "def reverse_transform(column_data):\n",
    "    return np.power(2,column_data.round())\n",
    "\n",
    "def transform_input(column_data):\n",
    "    return column_data/100000.0 \n",
    "def reverse_transform_input(column_data):\n",
    "    return 100000.0*column_data.round()\n",
    "\n",
    "from sdv.constraints import CustomConstraint, Between\n",
    "constraint = CustomConstraint(\n",
    "    columns=['p4','p5'],\n",
    "    transform=transform,\n",
    "    reverse_transform=reverse_transform\n",
    "    )\n",
    "\n",
    "constraint2 = CustomConstraint(\n",
    "    columns=['input'],\n",
    "    transform=transform_input,\n",
    "    reverse_transform=reverse_transform_input\n",
    "    )\n",
    "\n",
    "constraint_input = Between(\n",
    "    column='input',\n",
    "    low=100000,\n",
    "    high=10000001,\n",
    "    handling_strategy='transform'\n",
    "    )\n",
    "\n",
    "model = GaussianCopula(\n",
    "            field_names = ['input','p0','p1','p2','p3','p4','p5','p6','p7','speedup'],    \n",
    "            field_transformers = {'input': 'integer',\n",
    "                                  'p0': 'categorical',\n",
    "                                  'p1': 'categorical',\n",
    "                                  'p2': 'categorical',\n",
    "                                  'p3': 'categorical',\n",
    "                                  'p4': 'categorical',\n",
    "                                  'p5': 'categorical',\n",
    "                                  'p6': 'categorical', \n",
    "                                  'p7': 'categorical',\n",
    "                                  'speedup': 'float'},\n",
    "#             constraints=[constraint]\n",
    "            constraints=[constraint_input]\n",
    "    )\n",
    "model.fit(real_data)\n",
    "\n",
    "input_sizes = {}\n",
    "input_sizes['s']  = [100000] \n",
    "input_sizes['sm'] = [500000]\n",
    "input_sizes['m']  = [1000000]\n",
    "input_sizes['ml'] = [2500000]\n",
    "input_sizes['l']  = [5000000]\n",
    "input_sizes['xl'] = [10000000]\n",
    "# model.fit(real_data)\n",
    "conditions = pd.DataFrame({'input': [input_sizes['sm'][0],input_sizes['ml'][0],input_sizes['xl'][0]], 'speedup':[7.0,7.0,7.0]})\n",
    "\n",
    "conditions = {'input': input_sizes['sm'][0]} #, 'speedup':7.7}\n",
    "\n",
    "# conditions = pd.DataFrame({'gender': ['M', 'M', 'M', 'F', 'F', 'F']})\n",
    "ss2 = model.sample(1000,conditions=conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ss1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = model.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sdv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (sdv.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(real_data['input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ss2['speedup'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ss1['speedup'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(real_data['speedup'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(ss1['speedup'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #GPyOpt - Cases are important, for some reason\n",
    "# import GPyOpt\n",
    "# from GPyOpt.methods import BayesianOptimization\n",
    "# from collections import Counter, defaultdict\n",
    "# #numpy\n",
    "# import numpy as np\n",
    "# from numpy.random import multivariate_normal #For later example\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "#Plotting tools\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import numpy as np\n",
    "from numpy.random import multivariate_normal\n",
    "import matplotlib.ticker as mtick\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors.kde import KernelDensity\n",
    "# import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from autotune import TuningProblem\n",
    "from autotune.space import *\n",
    "import os, sys, time, json, math\n",
    "import ConfigSpace as CS\n",
    "import ConfigSpace.hyperparameters as CSH\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm\n",
    "import csv\n",
    "from csv import writer\n",
    "from csv import reader\n",
    "from sklearn import preprocessing\n",
    "HERE = os.path.dirname(os.path.abspath(__file__))\n",
    "sys.path.insert(1, os.path.dirname(HERE)+ '/plopper')\n",
    "from plopper import Plopper\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sdv.tabular import GaussianCopula\n",
    "from sdv.tabular import CopulaGAN\n",
    "from sdv.evaluation import evaluate\n",
    "from sdv.tabular import CTGAN\n",
    "from sdv.tabular import TVAE\n",
    "\n",
    "\n",
    "RANDOM_SEED = 1234\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "cs = CS.ConfigurationSpace(seed=RANDOM_SEED)\n",
    "# number of threads\n",
    "p0= CSH.OrdinalHyperparameter(name='p0', sequence=['2','3','4','5','6','7','8'], default_value='8')\n",
    "#block size for openmp dynamic schedule\n",
    "p1= CSH.OrdinalHyperparameter(name='p1', sequence=['10','20','40','64','80','100','128','160','200'], default_value='100')\n",
    "#clang unrolling\n",
    "p2= CSH.CategoricalHyperparameter(name='p2', choices=[\"#pragma clang loop unrolling full\", \" \"], default_value=' ')\n",
    "#omp parallel\n",
    "p3= CSH.CategoricalHyperparameter(name='p3', choices=[\"#pragma omp parallel for\", \" \"], default_value=' ')\n",
    "# tile size for one dimension for 2D tiling\n",
    "p4= CSH.OrdinalHyperparameter(name='p4', sequence=['2','4','8','16','32','64','96','128','256'], default_value='96')\n",
    "# tile size for another dimension for 2D tiling\n",
    "p5= CSH.OrdinalHyperparameter(name='p5', sequence=['2','4','8','16','32','64','96','128','256'], default_value='256')\n",
    "# omp placement\n",
    "p6= CSH.CategoricalHyperparameter(name='p6', choices=['cores','threads','sockets'], default_value='cores')\n",
    "p7= CSH.CategoricalHyperparameter(name='p7', choices=['compact','scatter','balanced','none','disabled', 'explicit'], default_value='none')\n",
    "\n",
    "cs.add_hyperparameters([p0, p1, p2, p3, p4, p5, p6, p7])\n",
    "\n",
    "dir_path = os.path.dirname(os.path.realpath(__file__))\n",
    "kernel_idx = dir_path.rfind('/')\n",
    "kernel = dir_path[kernel_idx+1:]\n",
    "obj = Plopper(dir_path+'/mmp.c',dir_path)\n",
    "\n",
    "x1=['p0','p1','p2','p3','p4','p5','p6','p7']\n",
    "exe_times = []\n",
    "def myobj(point: dict):\n",
    "\n",
    "  def plopper_func(x):\n",
    "    x = np.asarray_chkfinite(x)  # ValueError if any NaN or Inf\n",
    "    value = [point[x1[0]],point[x1[1]],point[x1[2]],point[x1[3]],point[x1[4]],point[x1[5]],point[x1[6]],point[x1[7]]] \n",
    "    print('VALUES:',point[x1[0]])\n",
    "    params = [\"P0\",\"P1\",\"P2\",\"P3\",\"P4\",\"P5\",\"P6\",\"P7\"]\n",
    "\n",
    "    result = obj.findRuntime(value, params, ' -s large -m event -l 500000') # defined(MINI_DATASET) && !defined(SMALL_DATASET) && !defined(MEDIUM_DATASET) && !defined(LARGE_DATASET) && !defined(EXTRALARGE_DATASET) && !defined(HUGE_DATASET)\n",
    "    return result\n",
    "\n",
    "  x = np.array([point[f'p{i}'] for i in range(len(point))])  \n",
    "  results = plopper_func(x)\n",
    "  exe_times.append(results)\n",
    "  np.save(dir_path+'/exe_times_sdv.npy',exe_times)\n",
    "#   results_s = sorted(results)\n",
    "#   results_m = results_s[1:-1]\n",
    "  print('OUTPUT:%f',results, float(np.mean(results[1:])))\n",
    "  return float(np.mean(results[1:]))\n",
    "\n",
    "df = pd.read_csv('./xsbench_input/results_rf_sml_xsbench_speedup.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "q_10 = np.quantile(df.objective.values, 0.9)\n",
    "print(q_10)\n",
    "q_10_s = np.quantile(df.speedup.values, 0.9)\n",
    "print(q_10_s)\n",
    "# real_df = df.loc[df['objective'] < q_10]\n",
    "real_df = df.loc[df['speedup'] > q_10_s]\n",
    "real_data = real_df.drop(columns=['elapsed_sec'])\n",
    "real_data = real_data.drop(columns=['objective'])\n",
    "# real_data = real_data.drop(columns=['speedup'])\n",
    "\n",
    "\n",
    "\n",
    "# max_speedup = np.max(real_data['speedup'])\n",
    "\n",
    "def transform(column_data):\n",
    "    return np.log2(column_data)\n",
    "def reverse_transform(column_data):\n",
    "    return np.power(2,column_data.round())\n",
    "\n",
    "def transform_input(column_data):\n",
    "    return column_data/100000.0 \n",
    "def reverse_transform_input(column_data):\n",
    "    return 100000.0*column_data.round()\n",
    "\n",
    "from sdv.constraints import CustomConstraint, Between\n",
    "constraint = CustomConstraint(\n",
    "    columns=['p4','p5'],\n",
    "    transform=transform,\n",
    "    reverse_transform=reverse_transform\n",
    "    )\n",
    "\n",
    "constraint2 = CustomConstraint(\n",
    "    columns=['input'],\n",
    "    transform=transform_input,\n",
    "    reverse_transform=reverse_transform_input\n",
    "    )\n",
    "\n",
    "constraint_input = Between(\n",
    "    column='input',\n",
    "    low=100000-2,\n",
    "    high=10000001,\n",
    "#     handling_strategy='transform'\n",
    "    )\n",
    "\n",
    "model = GaussianCopula(\n",
    "            field_names = ['input','p0','p1','p2','p3','p4','p5','p6','p7','speedup'],    \n",
    "            field_transformers = {'input': 'integer',\n",
    "                                  'p0': 'categorical',\n",
    "                                  'p1': 'categorical',\n",
    "                                  'p2': 'categorical',\n",
    "                                  'p3': 'categorical',\n",
    "                                  'p4': 'categorical',\n",
    "                                  'p5': 'categorical',\n",
    "                                  'p6': 'categorical', \n",
    "                                  'p7': 'categorical',\n",
    "                                  'speedup': 'float'},\n",
    "            constraints=[constraint_input]\n",
    "    )\n",
    "model.fit(real_data)\n",
    "\n",
    "print (model)\n",
    "\n",
    "input_sizes = {}\n",
    "input_sizes['s']  = [100000] \n",
    "input_sizes['sm'] = [500000]\n",
    "input_sizes['m']  = [1000000]\n",
    "input_sizes['ml'] = [2500000]\n",
    "input_sizes['l']  = [5000000]\n",
    "input_sizes['xl'] = [10000000]\n",
    "# model.fit(real_data)\n",
    "# conditions = pd.DataFrame({'input': [input_sizes['sm'][0],input_sizes['ml'][0],input_sizes['xl'][0]], 'speedup':[7.0,7.0,7.0]})\n",
    "\n",
    "conditions = {'input': input_sizes['xl'][0]}\n",
    "\n",
    "# conditions = pd.DataFrame({'gender': ['M', 'M', 'M', 'F', 'F', 'F']})\n",
    "ss1 = model.sample(10,conditions=conditions)\n",
    "\n",
    "# new_kde = \n",
    "\n",
    "# name of csv file \n",
    "filename = \"results_sdv.csv\"\n",
    "fields   = ['p0','p1','p2','p3','p4','p5','p6','p7','exe_time']#,'density']\n",
    "# fields   = ['p1','p2','p3','p4','p5','exe_time','density']\n",
    "# writing to csv file \n",
    "with open(filename, 'w') as csvfile: \n",
    "    # creating a csv writer object \n",
    "    csvwriter = csv.writer(csvfile) \n",
    "        \n",
    "    # writing the fields \n",
    "    csvwriter.writerow(fields) \n",
    "        \n",
    "    # writing the data rows \n",
    "#     csvwriter.writerows(rows)\n",
    "\n",
    "    evals_infer = []\n",
    "#     for idx in range(N_infer):\n",
    "    for row in ss1.iterrows():\n",
    "        sample_point_val = row[1].values[1:]\n",
    "        print (sample_point_val)\n",
    "        sample_point = {x1[0]:sample_point_val[0],\n",
    "                    x1[1]:sample_point_val[1],\n",
    "                    x1[2]:sample_point_val[2],\n",
    "                    x1[3]:sample_point_val[3],\n",
    "                    x1[4]:sample_point_val[4],\n",
    "                    x1[5]:sample_point_val[5],\n",
    "                    x1[6]:sample_point_val[6],\n",
    "                    x1[7]:sample_point_val[7]}\n",
    "        print (sample_point)\n",
    "        res          = myobj(sample_point)\n",
    "        print (sample_point, res)\n",
    "        evals_infer.append(res)\n",
    "#         ss = [sample_point['p0']] + [sample_point['p1']] + [sample_point['p2']] + [sample_point['p3']] +[sample_point['p4']]+[sample_point['p5']]+[sample_point['p6']]+[sample_point['p7']]+[res]+[new_kde[idx][1]]\n",
    "        ss = [sample_point['p0']] + [sample_point['p1']] + [sample_point['p2']] + [sample_point['p3']] +[sample_point['p4']]+[sample_point['p5']]+[sample_point['p6']]+[sample_point['p7']]+[res]+[sample_point_val[-1]]\n",
    "        csvwriter.writerow(ss)\n",
    "        csvfile.flush()\n",
    "csvfile.close()   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Plotting tools\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import numpy as np\n",
    "from numpy.random import multivariate_normal\n",
    "import matplotlib.ticker as mtick\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, random\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors.kde import KernelDensity\n",
    "# import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from autotune import TuningProblem\n",
    "from autotune.space import *\n",
    "import os, sys, time, json, math\n",
    "import ConfigSpace as CS\n",
    "import ConfigSpace.hyperparameters as CSH\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm\n",
    "import csv\n",
    "from csv import writer\n",
    "from csv import reader\n",
    "from sklearn import preprocessing\n",
    "# HERE = os.path.dirname(os.path.abspath(__file__))\n",
    "# sys.path.insert(1, os.path.dirname(HERE)+ '/plopper')\n",
    "# from plopper import Plopper\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sdv.tabular import GaussianCopula\n",
    "from sdv.tabular import CopulaGAN\n",
    "from sdv.evaluation import evaluate\n",
    "from sdv.tabular import CTGAN\n",
    "from sdv.tabular import TVAE\n",
    "\n",
    "RANDOM_SEED = 1234\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "cs = CS.ConfigurationSpace(seed=RANDOM_SEED)\n",
    "# number of threads\n",
    "p0= CSH.OrdinalHyperparameter(name='p0', sequence=['2','3','4','5','6','7','8'], default_value='8')\n",
    "#block size for openmp dynamic schedule\n",
    "p1= CSH.OrdinalHyperparameter(name='p1', sequence=['10','20','40','64','80','100','128','160','200'], default_value='100')\n",
    "#clang unrolling\n",
    "p2= CSH.CategoricalHyperparameter(name='p2', choices=[\"#pragma clang loop unrolling full\", \" \"], default_value=' ')\n",
    "#omp parallel\n",
    "p3= CSH.CategoricalHyperparameter(name='p3', choices=[\"#pragma omp parallel for\", \" \"], default_value=' ')\n",
    "# tile size for one dimension for 2D tiling\n",
    "p4= CSH.OrdinalHyperparameter(name='p4', sequence=['2','4','8','16','32','64','96','128','256'], default_value='96')\n",
    "# tile size for another dimension for 2D tiling\n",
    "p5= CSH.OrdinalHyperparameter(name='p5', sequence=['2','4','8','16','32','64','96','128','256'], default_value='256')\n",
    "# omp placement\n",
    "p6= CSH.CategoricalHyperparameter(name='p6', choices=['cores','threads','sockets'], default_value='cores')\n",
    "p7= CSH.CategoricalHyperparameter(name='p7', choices=['compact','scatter','balanced','none','disabled', 'explicit'], default_value='none')\n",
    "\n",
    "cs.add_hyperparameters([p0, p1, p2, p3, p4, p5, p6, p7])\n",
    "\n",
    "# dir_path = os.path.dirname(os.path.realpath(__file__))\n",
    "# kernel_idx = dir_path.rfind('/')\n",
    "# kernel = dir_path[kernel_idx+1:]\n",
    "# obj = Plopper(dir_path+'/mmp.c',dir_path)\n",
    "\n",
    "x1=['p0','p1','p2','p3','p4','p5','p6','p7']\n",
    "exe_times = []\n",
    "def myobj(point: dict):\n",
    "\n",
    "  def plopper_func(x):\n",
    "    x = np.asarray_chkfinite(x)  # ValueError if any NaN or Inf\n",
    "    value = [point[x1[0]],point[x1[1]],point[x1[2]],point[x1[3]],point[x1[4]],point[x1[5]],point[x1[6]],point[x1[7]]] \n",
    "    print('VALUES:',point[x1[0]])\n",
    "    params = [\"P0\",\"P1\",\"P2\",\"P3\",\"P4\",\"P5\",\"P6\",\"P7\"]\n",
    "\n",
    "    result = obj.findRuntime(value, params, ' -s large -m event -l 500000') # defined(MINI_DATASET) && !defined(SMALL_DATASET) && !defined(MEDIUM_DATASET) && !defined(LARGE_DATASET) && !defined(EXTRALARGE_DATASET) && !defined(HUGE_DATASET)\n",
    "    return result\n",
    "\n",
    "  x = np.array([point[f'p{i}'] for i in range(len(point))])  \n",
    "  results = plopper_func(x)\n",
    "  exe_times.append(results)\n",
    "  np.save(dir_path+'/exe_times_kde.npy',exe_times)\n",
    "#   results_s = sorted(results)\n",
    "#   results_m = results_s[1:-1]\n",
    "  print('OUTPUT:%f',results, float(np.mean(results[1:])))\n",
    "  return float(np.mean(results[1:]))\n",
    "\n",
    "param_names = cs.get_hyperparameter_names()\n",
    "\n",
    "## add input\n",
    "param_vals = [] \n",
    "param_obj = {}\n",
    "input_sizes = {}\n",
    "input_sizes['s']  = [100000] \n",
    "input_sizes['sm'] = [500000]\n",
    "input_sizes['m']  = [1000000]\n",
    "input_sizes['ml'] = [2500000]\n",
    "input_sizes['l']  = [5000000]\n",
    "input_sizes['xl'] = [10000000]\n",
    "## add inputs sizes \n",
    "vals = [input_sizes['s'][0], input_sizes['xl'][0]]\n",
    "param_vals.append(vals)\n",
    "X = np.array(vals) \n",
    "X = X[:,np.newaxis]\n",
    "transformer = preprocessing.MinMaxScaler().fit(X)\n",
    "param_obj['input'] = transformer\n",
    "print (param_obj)\n",
    "# #### selet by best top x%\n",
    "if False:\n",
    "    take_n = int(len(y_eval) * 0.1)\n",
    "    take_idx = np.argsort(y_eval)[-take_n:]\n",
    "    X_opt = X_eval[take_idx]\n",
    "    print (X_opt)   \n",
    "else:\n",
    "    X_opt = []\n",
    "    cutoff_p = 0.1\n",
    "\n",
    "    '''\n",
    "    #### problem       S       L         XL       XXL\n",
    "    size (s)      :    12      355       355      355        - nuclides\n",
    "    gridpoints (g):  11,303   11,303   238,847   501,578     - grid points per nuclide\n",
    "    particle   (p): 500,000  500,000   500,000   500,000     -  \n",
    "    lookup     (l):                                          - \n",
    "    '''\n",
    "    n_param = len(param_names)\n",
    "    frames = []\n",
    "    for i_size, o3p_tmp in zip(['s','m','l'],[0.297755, 3.00738, 15.0962]):#['s','m','l']: 0.00106, 0.0266395, 3.972039\n",
    "        dataframe = pd.read_csv(\"results_rf_\"+str(i_size)+\"_xsbench.csv\") # PROBLEM_SIZE\tBLOCK_SIZE\texe_time\tLOG(exe_time)\tspeedup\telapsed_sec \n",
    "#         col_speedup = dataframe['objective'] / o3p_tmp\n",
    "        \n",
    "        dataframe['speedup'] = o3p_tmp / dataframe['objective']\n",
    "        dataframe['input']   = pd.Series(input_sizes[i_size][0] for _ in range(len(dataframe.index)))\n",
    "        q_10_s = np.quantile(dataframe.speedup.values, 0.9)\n",
    "        real_df = dataframe.loc[dataframe['speedup'] >= q_10_s]\n",
    "        real_data = real_df.drop(columns=['elapsed_sec'])\n",
    "        real_data = real_data.drop(columns=['objective'])\n",
    "        print (i_size, input_sizes[i_size][0], len(real_data),q_10_s)\n",
    "        frames.append(real_data)\n",
    "        \n",
    "real_data = pd.concat(frames)        \n",
    "max_speedup = np.max(real_data['speedup'])\n",
    "\n",
    "def transform(column_data):\n",
    "    return np.log2(column_data)\n",
    "def reverse_transform(column_data):\n",
    "    return np.power(2,column_data.round())\n",
    "\n",
    "def transform_input(column_data):\n",
    "    return column_data/100000.0 \n",
    "def reverse_transform_input(column_data):\n",
    "    return 100000.0*column_data.round()\n",
    "\n",
    "from sdv.constraints import CustomConstraint, Between\n",
    "constraint = CustomConstraint(\n",
    "    columns=['p4','p5'],\n",
    "    transform=transform,\n",
    "    reverse_transform=reverse_transform\n",
    "    )\n",
    "\n",
    "constraint2 = CustomConstraint(\n",
    "    columns=['input'],\n",
    "    transform=transform_input,\n",
    "    reverse_transform=reverse_transform_input\n",
    "    )\n",
    "\n",
    "constraint_input = Between(\n",
    "    column='input',\n",
    "    low=100000,\n",
    "    high=10000001,\n",
    "#     handling_strategy='transform'\n",
    "    )\n",
    "\n",
    "model = GaussianCopula(\n",
    "            field_names = ['input','p0','p1','p2','p3','p4','p5','p6','p7','speedup'],    \n",
    "            field_transformers = {'input': 'integer',\n",
    "                                  'p0': 'categorical',\n",
    "                                  'p1': 'categorical',\n",
    "                                  'p2': 'categorical',\n",
    "                                  'p3': 'categorical',\n",
    "                                  'p4': 'categorical',\n",
    "                                  'p5': 'categorical',\n",
    "                                  'p6': 'categorical', \n",
    "                                  'p7': 'categorical',\n",
    "                                  'speedup': 'float'},\n",
    "            constraints=[constraint_input]\n",
    "    )\n",
    "model.fit(real_data)\n",
    "\n",
    "print (model)\n",
    "\n",
    "input_sizes = {}\n",
    "input_sizes['s']  = [100000] \n",
    "input_sizes['sm'] = [500000]\n",
    "input_sizes['m']  = [1000000]\n",
    "input_sizes['ml'] = [2500000]\n",
    "input_sizes['l']  = [5000000]\n",
    "input_sizes['xl'] = [10000000]\n",
    "# model.fit(real_data)\n",
    "# conditions = pd.DataFrame({'input': [input_sizes['sm'][0],input_sizes['ml'][0],input_sizes['xl'][0]], 'speedup':[7.0,7.0,7.0]})\n",
    "\n",
    "conditions = {'input': input_sizes['sm'][0]}\n",
    "\n",
    "# conditions = pd.DataFrame({'gender': ['M', 'M', 'M', 'F', 'F', 'F']})\n",
    "ss1 = model.sample(conditions=conditions)\n",
    "\n",
    "# # new_kde = \n",
    "\n",
    "# # name of csv file \n",
    "# filename = \"results_sdv.csv\"\n",
    "# fields   = ['p0','p1','p2','p3','p4','p5','p6','p7','exe_time']#,'density']\n",
    "# # fields   = ['p1','p2','p3','p4','p5','exe_time','density']\n",
    "# # writing to csv file \n",
    "# with open(filename, 'w') as csvfile: \n",
    "#     # creating a csv writer object \n",
    "#     csvwriter = csv.writer(csvfile) \n",
    "        \n",
    "#     # writing the fields \n",
    "#     csvwriter.writerow(fields) \n",
    "        \n",
    "#     # writing the data rows \n",
    "# #     csvwriter.writerows(rows)\n",
    "\n",
    "#     evals_infer = []\n",
    "# #     for idx in range(N_infer):\n",
    "#     for row in ss1.iterrows():\n",
    "#         sample_point_val = row[1].values[1:]\n",
    "#         print (sample_point_val)\n",
    "#         sample_point = {x1[0]:sample_point_val[0],\n",
    "#                     x1[1]:sample_point_val[1],\n",
    "#                     x1[2]:sample_point_val[2],\n",
    "#                     x1[3]:sample_point_val[3],\n",
    "#                     x1[4]:sample_point_val[4],\n",
    "#                     x1[5]:sample_point_val[5],\n",
    "#                     x1[6]:sample_point_val[6],\n",
    "#                     x1[7]:sample_point_val[7]}\n",
    "#         print (sample_point)\n",
    "#         res          = myobj(sample_point)\n",
    "#         print (sample_point, res)\n",
    "#         evals_infer.append(res)\n",
    "# #         ss = [sample_point['p0']] + [sample_point['p1']] + [sample_point['p2']] + [sample_point['p3']] +[sample_point['p4']]+[sample_point['p5']]+[sample_point['p6']]+[sample_point['p7']]+[res]+[new_kde[idx][1]]\n",
    "#         ss = [sample_point['p0']] + [sample_point['p1']] + [sample_point['p2']] + [sample_point['p3']] +[sample_point['p4']]+[sample_point['p5']]+[sample_point['p6']]+[sample_point['p7']]+[res]+[sample_point_val[-1]]\n",
    "#         csvwriter.writerow(ss)\n",
    "#         csvfile.flush()\n",
    "# csvfile.close()           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = ss1.sort_values(by='speedup', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ss1['speedup'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(ss1['speedup']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(ss1['speedup'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting tools\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import numpy as np\n",
    "from numpy.random import multivariate_normal\n",
    "import matplotlib.ticker as mtick\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, random\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors.kde import KernelDensity\n",
    "# import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from autotune import TuningProblem\n",
    "from autotune.space import *\n",
    "import os, sys, time, json, math\n",
    "import ConfigSpace as CS\n",
    "import ConfigSpace.hyperparameters as CSH\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm\n",
    "import csv, time \n",
    "from csv import writer\n",
    "from csv import reader\n",
    "from sklearn import preprocessing\n",
    "# HERE = os.path.dirname(os.path.abspath(__file__))\n",
    "# sys.path.insert(1, os.path.dirname(HERE)+ '/plopper')\n",
    "# from plopper import Plopper\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sdv.tabular import GaussianCopula\n",
    "from sdv.tabular import CopulaGAN\n",
    "from sdv.evaluation import evaluate\n",
    "from sdv.tabular import CTGAN\n",
    "from sdv.tabular import TVAE\n",
    "\n",
    "RANDOM_SEED = 1234\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "def pretty_time(seconds):\n",
    "    \"\"\"Format time string\"\"\"\n",
    "    seconds = round(seconds, 2)\n",
    "    minutes, seconds = divmod(seconds, 60)\n",
    "    hours, minutes = divmod(minutes, 60)\n",
    "    return \"%02d:%02d:%02.2f\" % (hours,minutes,seconds)\n",
    "\n",
    "Time_start = time.time()\n",
    "print ('time...now', Time_start)\n",
    "cs = CS.ConfigurationSpace(seed=1234)\n",
    "# number of threads\n",
    "p0= CSH.OrdinalHyperparameter(name='p0', sequence=['4','5','6','7','8'], default_value='8')\n",
    "#block size for openmp dynamic schedule\n",
    "p1= CSH.OrdinalHyperparameter(name='p1', sequence=['100','200','400','640','800','1000','1280','1600','2000'], default_value='1000')\n",
    "#clang unrolling\n",
    "p2= CSH.CategoricalHyperparameter(name='p2', choices=[\"#pragma clang loop unrolling full\", \" \"], default_value=' ')\n",
    "#omp parallel\n",
    "p3= CSH.CategoricalHyperparameter(name='p3', choices=[\"#pragma omp parallel for\", \" \"], default_value=' ')\n",
    "# tile size for one dimension for 2D tiling\n",
    "p4= CSH.OrdinalHyperparameter(name='p4', sequence=['2','4','8','16','32','64','96','128','256'], default_value='96')\n",
    "# tile size for another dimension for 2D tiling\n",
    "p5= CSH.OrdinalHyperparameter(name='p5', sequence=['2','4','8','16','32','64','96','128','256'], default_value='256')\n",
    "p6= CSH.OrdinalHyperparameter(name='p6', sequence=['10','20','40','64','80','100','128','160','200'], default_value='100')\n",
    "#thread affinity type\n",
    "p7= CSH.CategoricalHyperparameter(name='p7', choices=['compact','scatter','balanced','none','disabled', 'explicit'], default_value='none')\n",
    "# omp placement\n",
    "p8= CSH.CategoricalHyperparameter(name='p8', choices=['cores','threads','sockets'], default_value='cores')\n",
    "\n",
    "cs.add_hyperparameters([p0, p1, p2, p3, p4, p5, p6, p7, p8])\n",
    "\n",
    "# dir_path = os.path.dirname(os.path.realpath(__file__))\n",
    "# kernel_idx = dir_path.rfind('/')\n",
    "# kernel = dir_path[kernel_idx+1:]\n",
    "# obj = Plopper(dir_path+'/mmp.c',dir_path)\n",
    "\n",
    "x1=['p0','p1','p2','p3','p4','p5','p6','p7','p8']\n",
    "exe_times = []\n",
    "def myobj(point: dict):\n",
    "\n",
    "  def plopper_func(x):\n",
    "    x = np.asarray_chkfinite(x)  # ValueError if any NaN or Inf\n",
    "    value = [point[x1[0]],point[x1[1]],point[x1[2]],point[x1[3]],point[x1[4]],point[x1[5]],point[x1[6]],point[x1[7]],point[x1[8]]]\n",
    "    print('VALUES:',point[x1[0]])\n",
    "    params = [\"P0\",\"P1\",\"P2\",\"P3\",\"P4\",\"P5\",\"P6\",\"P7\",\"P8\"]\n",
    "\n",
    "    result = obj.findRuntime(value, params, ' -s large -m event -l 500000') # defined(MINI_DATASET) && !defined(SMALL_DATASET) && !defined(MEDIUM_DATASET) && !defined(LARGE_DATASET) && !defined(EXTRALARGE_DATASET) && !defined(HUGE_DATASET)\n",
    "    return result\n",
    "\n",
    "  x = np.array([point[f'p{i}'] for i in range(len(point))])  \n",
    "  results = plopper_func(x)\n",
    "  exe_times.append(results)\n",
    "  np.save(dir_path+'/exe_times_sdv.npy',exe_times)\n",
    "#   results_s = sorted(results)\n",
    "#   results_m = results_s[1:-1]\n",
    "  print('OUTPUT:%f',results, float(np.mean(results[1:])))\n",
    "  return float(np.mean(results[1:]))\n",
    "\n",
    "param_names = cs.get_hyperparameter_names()\n",
    "\n",
    "## add input\n",
    "param_vals = [] \n",
    "param_obj = {}\n",
    "input_sizes = {}\n",
    "input_sizes['s']  = [100000] \n",
    "input_sizes['sm'] = [500000]\n",
    "input_sizes['m']  = [1000000]\n",
    "input_sizes['ml'] = [2500000]\n",
    "input_sizes['l']  = [5000000]\n",
    "input_sizes['xl'] = [10000000]\n",
    "## add inputs sizes \n",
    "vals = [input_sizes['s'][0], input_sizes['xl'][0]]\n",
    "param_vals.append(vals)\n",
    "X = np.array(vals) \n",
    "X = X[:,np.newaxis]\n",
    "transformer = preprocessing.MinMaxScaler().fit(X)\n",
    "param_obj['input'] = transformer\n",
    "print (param_obj)\n",
    "# #### selet by best top x%\n",
    "if False:\n",
    "    take_n = int(len(y_eval) * 0.1)\n",
    "    take_idx = np.argsort(y_eval)[-take_n:]\n",
    "    X_opt = X_eval[take_idx]\n",
    "    print (X_opt)   \n",
    "else:\n",
    "    X_opt = []\n",
    "    cutoff_p = 0.1\n",
    "\n",
    "    '''\n",
    "    #### problem       S       L         XL       XXL\n",
    "    size (s)      :    12      355       355      355        - nuclides\n",
    "    gridpoints (g):  11,303   11,303   238,847   501,578     - grid points per nuclide\n",
    "    particle   (p): 500,000  500,000   500,000   500,000     -  \n",
    "    lookup     (l):                                          - \n",
    "    '''\n",
    "    n_param = len(param_names)\n",
    "    frames = []\n",
    "    for i_size, o3p_tmp in zip(['s','m','l'],[1.7527, 17.7599, 88.3151]):#['s','m','l']: 0.00106, 0.0266395, 3.972039\n",
    "        dataframe = pd.read_csv(\"results_rf_\"+str(i_size)+\"_rsbench.csv\") # PROBLEM_SIZE\tBLOCK_SIZE\texe_time\tLOG(exe_time)\tspeedup\telapsed_sec \n",
    "#         col_speedup = dataframe['objective'] / o3p_tmp\n",
    "        dataframe['speedup'] = o3p_tmp / dataframe['objective']\n",
    "        dataframe['input']   = pd.Series(input_sizes[i_size][0] for _ in range(len(dataframe.index)))\n",
    "        q_10_s = np.quantile(dataframe.speedup.values, 0.9)\n",
    "        real_df = dataframe.loc[dataframe['speedup'] >= q_10_s]\n",
    "        real_data = real_df.drop(columns=['elapsed_sec'])\n",
    "        real_data = real_data.drop(columns=['objective'])\n",
    "        print (i_size, input_sizes[i_size][0], len(real_data),q_10_s)\n",
    "        frames.append(real_data)\n",
    "        \n",
    "real_data = pd.concat(frames)        \n",
    "max_speedup = np.max(real_data['speedup'])\n",
    "\n",
    "def transform(column_data):\n",
    "    return np.log2(column_data)\n",
    "def reverse_transform(column_data):\n",
    "    return np.power(2,column_data.round())\n",
    "\n",
    "def transform_input(column_data):\n",
    "    return column_data/100000.0 \n",
    "def reverse_transform_input(column_data):\n",
    "    return 100000.0*column_data.round()\n",
    "\n",
    "from sdv.constraints import CustomConstraint, Between\n",
    "constraint = CustomConstraint(\n",
    "    columns=['p4','p5'],\n",
    "    transform=transform,\n",
    "    reverse_transform=reverse_transform\n",
    "    )\n",
    "\n",
    "constraint2 = CustomConstraint(\n",
    "    columns=['input'],\n",
    "    transform=transform_input,\n",
    "    reverse_transform=reverse_transform_input\n",
    "    )\n",
    "\n",
    "constraint_input = Between(\n",
    "    column='input',\n",
    "    low=100000-100,\n",
    "    high=10000000+100,\n",
    "#     handling_strategy='transform'\n",
    "    )\n",
    "\n",
    "model = GaussianCopula(\n",
    "            field_names = ['input','p0','p1','p2','p3','p4','p5','p6','p7','p8','speedup'],    \n",
    "            field_transformers = {'input': 'integer',\n",
    "                                  'p0': 'categorical',\n",
    "                                  'p1': 'categorical',\n",
    "                                  'p2': 'categorical',\n",
    "                                  'p3': 'categorical',\n",
    "                                  'p4': 'categorical',\n",
    "                                  'p5': 'categorical',\n",
    "                                  'p6': 'categorical', \n",
    "                                  'p7': 'categorical',\n",
    "                                  'p8': 'categorical',\n",
    "                                  'speedup': 'float'},\n",
    "            constraints=[constraint_input]\n",
    "    )\n",
    "model.fit(real_data)\n",
    "\n",
    "print (model)\n",
    "\n",
    "input_sizes = {}\n",
    "input_sizes['s']  = [100000] \n",
    "input_sizes['sm'] = [500000]\n",
    "input_sizes['m']  = [1000000]\n",
    "input_sizes['ml'] = [2500000]\n",
    "input_sizes['l']  = [5000000]\n",
    "input_sizes['xl'] = [10000000]\n",
    "# model.fit(real_data)\n",
    "# conditions = pd.DataFrame({'input': [input_sizes['sm'][0],input_sizes['ml'][0],input_sizes['xl'][0]], 'speedup':[7.0,7.0,7.0]})\n",
    "\n",
    "conditions = {'input': input_sizes['sm'][0]}\n",
    "\n",
    "# conditions = pd.DataFrame({'gender': ['M', 'M', 'M', 'F', 'F', 'F']})\n",
    "ss1 = model.sample(1000,conditions=conditions)#,float_rtol=1.0)\n",
    "\n",
    "ss = ss1.sort_values(by='speedup', ascending=False)\n",
    "# ss.drop_duplicates()\n",
    "new_kde = ss[:200]\n",
    "# new_kde = \n",
    "print (new_kde['speedup'])\n",
    "# name of csv file \n",
    "filename = \"results_sdv.csv\"\n",
    "fields   = ['p0','p1','p2','p3','p4','p5','p6','p7','p8','exe_time','predictedsp','elapsed_sec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ytune",
   "language": "python",
   "name": "ytune"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
