{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting tools\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import numpy as np\n",
    "from numpy.random import multivariate_normal\n",
    "import matplotlib.ticker as mtick\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors.kde import KernelDensity\n",
    "# import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from autotune import TuningProblem\n",
    "from autotune.space import *\n",
    "import os, sys, time, json, math\n",
    "import ConfigSpace as CS\n",
    "import ConfigSpace.hyperparameters as CSH\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm\n",
    "import csv\n",
    "from csv import writer\n",
    "from csv import reader\n",
    "from sklearn import preprocessing\n",
    "# HERE = os.path.dirname(os.path.abspath(__file__))\n",
    "# sys.path.insert(1, os.path.dirname(HERE)+ '/plopper')\n",
    "# from plopper import Plopper\n",
    "\n",
    "cs = CS.ConfigurationSpace(seed=1234)\n",
    "# number of threads\n",
    "p0= CSH.OrdinalHyperparameter(name='p0', sequence=['2','3','4','5','6','7','8'], default_value='8')\n",
    "#block size for openmp dynamic schedule\n",
    "p1= CSH.OrdinalHyperparameter(name='p1', sequence=['10','20','40','64','80','100','128','160','200'], default_value='100')\n",
    "#clang unrolling\n",
    "p2= CSH.CategoricalHyperparameter(name='p2', choices=[\"#pragma clang loop unrolling full\", \" \"], default_value=' ')\n",
    "#omp parallel\n",
    "p3= CSH.CategoricalHyperparameter(name='p3', choices=[\"#pragma omp parallel for\", \" \"], default_value=' ')\n",
    "# tile size for one dimension for 2D tiling\n",
    "p4= CSH.OrdinalHyperparameter(name='p4', sequence=['2','4','8','16','32','64','96','128','256'], default_value='96')\n",
    "# tile size for another dimension for 2D tiling\n",
    "p5= CSH.OrdinalHyperparameter(name='p5', sequence=['2','4','8','16','32','64','96','128','256'], default_value='256')\n",
    "# omp placement\n",
    "p6= CSH.CategoricalHyperparameter(name='p6', choices=['cores','threads','sockets'], default_value='cores')\n",
    "p7= CSH.CategoricalHyperparameter(name='p7', choices=['compact','scatter','balanced','none','disabled', 'explicit'], default_value='none')\n",
    "\n",
    "cs.add_hyperparameters([p0, p1, p2, p3, p4, p5, p6, p7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# min_max_scaler = preprocessing.MinMaxScaler()\n",
    "# X_train_minmax = min_max_scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_infer = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param_names = cs.get_hyperparameter_names()\n",
    "param_vals = [] \n",
    "param_obj = {}\n",
    "for i, p_name in enumerate(param_names): #cs.get_hyperparameter_names()\n",
    "#     print (i,p_name)\n",
    "    try: # ordinal \n",
    "        vals = list(map(int,list(cs.get_hyperparameters()[i].sequence)))\n",
    "        param_vals.append(vals)\n",
    "        X = np.array(vals) \n",
    "        X = X[:,np.newaxis]\n",
    "        transformer = preprocessing.MinMaxScaler().fit(X)\n",
    "        param_obj[p_name] = transformer\n",
    "    except: ## norminal \n",
    "        vals = list(cs.get_hyperparameters()[i].choices)\n",
    "#         print (vals)\n",
    "        param_vals.append(vals)\n",
    "        X = np.array(vals) \n",
    "        X = X[:,np.newaxis]\n",
    "        transformer = preprocessing.OneHotEncoder(drop='if_binary',sparse=False).fit(X)\n",
    "        param_obj[p_name] = transformer\n",
    "## add input\n",
    "input_sizes = {}\n",
    "input_sizes['s']  = [100000]  \n",
    "input_sizes['m']  = [1000000]  \n",
    "input_sizes['l']  = [5000000]\n",
    "input_sizes['sm'] = [500000]\n",
    "input_sizes['ml'] = [2500000]\n",
    "input_sizes['xl'] = [10000000]\n",
    "## add inputs sizes \n",
    "vals = [input_sizes['s'][0], input_sizes['xl'][0]]\n",
    "param_vals.append(vals)\n",
    "X = np.array(vals) \n",
    "X = X[:,np.newaxis]\n",
    "transformer = preprocessing.MinMaxScaler().fit(X)\n",
    "param_obj['input'] = transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode = param_obj['input'].inverse_transform(np.array(np.array([[0.5],[0.5]])))\n",
    "te = decode #np.array([[' '],[' '],['#pragma omp parallel for']])\n",
    "coded = param_obj['input'].transform(te)\n",
    "decode = param_obj['p3'].inverse_transform(np.array([[1],[0]]))\n",
    "te = np.array([[' '],[' '],['#pragma omp parallel for']])\n",
    "coded = param_obj['p3'].transform(te)\n",
    "decode = param_obj['p6'].inverse_transform(np.array([[1,0,0]]))\n",
    "te = np.array([['cores'],['threads'],['sockets']])\n",
    "coded = param_obj['p6'].transform(te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coded_te = param_obj['p6'].inverse_transform(coded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coded_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #### selet by best top x%\n",
    "if False:\n",
    "    take_n = int(len(y_eval) * 0.1)\n",
    "    take_idx = np.argsort(y_eval)[-take_n:]\n",
    "    X_opt = X_eval[take_idx]\n",
    "    print (X_opt)   \n",
    "else:\n",
    "    X_opt = []\n",
    "    cutoff_p = 0.1\n",
    "\n",
    "    '''\n",
    "    #### problem       S       L         XL       XXL\n",
    "    size (s)      :    12      355       355      355        - nuclides\n",
    "    gridpoints (g):  11,303   11,303   238,847   501,578     - grid points per nuclide\n",
    "    particle   (p): 500,000  500,000   500,000   500,000     -  \n",
    "    lookup     (l):                                          - \n",
    "    '''\n",
    "    n_param = len(param_names)\n",
    "    for i_size, o3p_tmp in zip(['s','m','l'],[0.297755, 3.00738, 15.0962]):#['s','m','l']: 0.00106, 0.0266395, 3.972039\n",
    "        dataframe = pd.read_csv(\"results_rf_\"+str(i_size)+\"_xsbench.csv\") # PROBLEM_SIZE\tBLOCK_SIZE\texe_time\tLOG(exe_time)\tspeedup\telapsed_sec \n",
    "        array = dataframe.values\n",
    "        X_eval = array[:,:n_param]#.astype(float) ## ### size, n1, n2, p1,p2,p3,p4,p5,exe_time,speedup       \n",
    "#         X_eval_encode = np.copy(X_eval)\n",
    "        X_eval = np.c_[X_eval, np.array([input_sizes[i_size]]*len(X_eval))]\n",
    "        X_eval_encode = []\n",
    "        for i, p_name in enumerate(param_obj.keys()):\n",
    "            x_tmp = X_eval[:,i]\n",
    "            x_tmp = x_tmp[:,np.newaxis]\n",
    "            X_eval_encode.append(param_obj[p_name].transform(x_tmp)) ## [0,1]\n",
    "        X_eval_en = X_eval_encode[0] \n",
    "        for i in range(1,len(X_eval_encode)):    \n",
    "            X_eval_en = np.c_[X_eval_en,X_eval_encode[i]]\n",
    "                \n",
    "        y_eval = o3p_tmp / array[:,n_param].astype(float) ## speedup \n",
    "        take_n = round(len(y_eval) * cutoff_p)\n",
    "        take_idx = np.argsort(y_eval)[-take_n:]\n",
    "        X_opt.extend(X_eval_en[take_idx].astype(float))\n",
    "    print (X_opt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_opt_norm = X_opt\n",
    "\n",
    "colors = ['navy']#, 'cornflowerblue']#, 'darkorange']\n",
    "kernels = ['gaussian']#, 'tophat']#, 'epanechnikov']\n",
    "\n",
    "grid = GridSearchCV(KernelDensity(),\n",
    "                    {'bandwidth': np.linspace(0.001, 10, 10000)},\n",
    "                    cv=max(2,round(len(X_opt_norm)*0.2))) # 20-fold cross-validation\n",
    "\n",
    "grid.fit(X_opt_norm)\n",
    "print ('grid.best_params_',grid.best_params_)\n",
    "\n",
    "bandwidth_size = grid.best_params_['bandwidth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_plot_cs = cs.sample_configuration(N_infer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_plot = [] \n",
    "for c in X_plot_cs:\n",
    "    value = list(dict(c).values())\n",
    "    X_plot.append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_plot_0 = []\n",
    "X_plot = np.c_[X_plot, np.array([input_sizes['xl']]*len(X_plot))]\n",
    "for i, p_name in enumerate(param_obj.keys()):\n",
    "    x_tmp = X_plot[:,i]\n",
    "    x_tmp = x_tmp[:,np.newaxis]\n",
    "    X_plot_0.append(param_obj[p_name].transform(x_tmp)) ## [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_plot_norm = X_plot_0[0] \n",
    "for i in range(1,len(X_plot_0)):    \n",
    "    X_plot_norm = np.c_[X_plot_norm,X_plot_0[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lw = 2\n",
    "for color, kernel in zip(colors, kernels):\n",
    "    kde = KernelDensity(kernel=kernel, bandwidth=bandwidth_size).fit(X_opt_norm)\n",
    "    X_te = []\n",
    "    for i in range(len(X_plot_norm)):\n",
    "            log_den = kde.score_samples(np.array([X_plot_norm[i]]))\n",
    "            X_te.append([X_plot[i],np.exp(log_den[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.copy(X_te)\n",
    "take_n_te = N_infer\n",
    "take_idx = np.argsort(-1*np.array(X_test)[:,1])[:take_n_te]\n",
    "X_opt_te = []# np.array(X_test)[take_idx]\n",
    "for idx in take_idx:\n",
    "    x_sample = X_test[idx]\n",
    "    X_opt_te.append([x_sample[0],x_sample[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_kde = X_opt_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HERE = os.path.dirname(os.path.abspath(__file__))\n",
    "sys.path.insert(1, os.path.dirname(HERE)+ '/plopper')\n",
    "from plopper import Plopper\n",
    "\n",
    "\n",
    "dir_path = os.path.dirname(os.path.realpath(__file__))\n",
    "kernel_idx = dir_path.rfind('/')\n",
    "kernel = dir_path[kernel_idx+1:]\n",
    "obj = Plopper(dir_path+'/mmp.c',dir_path)\n",
    "\n",
    "x1=['p0','p1','p2','p3','p4','p5','p6','p7']\n",
    "exe_times = []\n",
    "def myobj(point: dict):\n",
    "\n",
    "  def plopper_func(x):\n",
    "    x = np.asarray_chkfinite(x)  # ValueError if any NaN or Inf\n",
    "    value = [point[x1[0]],point[x1[1]],point[x1[2]],point[x1[3]],point[x1[4]],point[x1[5]],point[x1[6]],point[x1[7]]] \n",
    "    print('VALUES:',point[x1[0]])\n",
    "    params = [\"P0\",\"P1\",\"P2\",\"P3\",\"P4\",\"P5\",\"P6\",\"P7\"]\n",
    "    value = list(point.values())\n",
    "    params = {k.upper(): v for k, v in point.items()}\n",
    "    result = obj.findRuntime(value, params, ' -s large -m event -l 10000000') # defined(MINI_DATASET) && !defined(SMALL_DATASET) && !defined(MEDIUM_DATASET) && !defined(LARGE_DATASET) && !defined(EXTRALARGE_DATASET) && !defined(HUGE_DATASET)\n",
    "    return result\n",
    "\n",
    "  x = np.array(list(point.values())) #np.array([point[f'p{i}'] for i in range(len(point))])  \n",
    "  results = plopper_func(x)\n",
    "  exe_times.append(results)\n",
    "  np.save(dir_path+'/exe_times.npy',exe_times)\n",
    "#   results_s = sorted(results)\n",
    "#   results_m = results_s[1:-1]\n",
    "  print('OUTPUT:%f',results, float(np.mean(results[1:])))\n",
    "  return float(np.mean(results[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name of csv file \n",
    "filename = \"results_kde.csv\"\n",
    "fields   = ['p0','p1','p2','p3','p4','p5','p6','p7','exe_time','density']\n",
    "# fields   = ['p1','p2','p3','p4','p5','exe_time','density']\n",
    "# writing to csv file \n",
    "with open(filename, 'w') as csvfile: \n",
    "    # creating a csv writer object \n",
    "    csvwriter = csv.writer(csvfile) \n",
    "        \n",
    "    # writing the fields \n",
    "    csvwriter.writerow(fields) \n",
    "        \n",
    "    # writing the data rows \n",
    "#     csvwriter.writerows(rows)\n",
    "\n",
    "    evals_infer = []\n",
    "    for idx in range(N_infer):\n",
    "        sample_point_val = new_kde[idx][0][-n_param:]\n",
    "        print (sample_point_val)\n",
    "        sample_point = {x1[0]:sample_point_val[0],\n",
    "                    x1[1]:sample_point_val[1],\n",
    "                    x1[2]:sample_point_val[2],\n",
    "                    x1[3]:sample_point_val[3],\n",
    "                    x1[4]:sample_point_val[4],\n",
    "                    x1[5]:sample_point_val[5],\n",
    "                    x1[6]:sample_point_val[6],\n",
    "                    x1[7]:sample_point_val[7]}\n",
    "#                     x1[8]:sample_point_val[8],\n",
    "#                     x1[9]:sample_point_val[9]}\n",
    "        res          = myobj(sample_point)\n",
    "        print (sample_point, res)\n",
    "        evals_infer.append(res)\n",
    "        ss = [sample_point['p0']] + [sample_point['p1']] + [sample_point['p2']] + [sample_point['p3']] +[sample_point['p4']]+[sample_point['p5']]+[sample_point['p6']]+[sample_point['p7']]+[res]+[new_kde[idx][1]]\n",
    "        csvwriter.writerow(ss)\n",
    "        csvfile.flush()\n",
    "csvfile.close()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [       1,     1000,    10000,   100000,  1000000,   5000000,  10000000,  15000000,  20000000]\n",
    "y = [0.000001, 0.002513, 0.028749, 0.297755, 3.007381, 15.096215, 30.048476, 45.241969, 60.167836] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import module\n",
    "from tabulate import tabulate\n",
    "  \n",
    "# assign data\n",
    "mydata = [{x[0], y[0]}, \n",
    "          {x[1], y[1]},\n",
    "          {x[2], y[2]},\n",
    "          {x[3], y[3]},\n",
    "          {x[4], y[4]},\n",
    "          {x[5], y[5]},\n",
    "          {x[6], y[6]},\n",
    "          {x[7], y[7]},\n",
    "          {x[8], y[8]},\n",
    "         ]\n",
    "  \n",
    "# create header\n",
    "head = [\"No. lookups\", \"execution time (sec.)\"]\n",
    "  \n",
    "# display table\n",
    "print(tabulate(mydata, headers=head, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(x, y, 'ro')\n",
    "plt.title('XSBench')\n",
    "plt.xlabel('The number of lookups')\n",
    "plt.ylabel('Execution time (sec.)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = [       1,     1000,    10000,   100000,   1000000,   5000000,   10000000,   15000000, 20000000]\n",
    "x = [ '1','1,000','10,000','100,000','1,000,000','5,000,000','10,000,000','15,000,000','20,000,000']\n",
    "y = [0.000002, 0.019164, 0.172359, 1.752702, 17.759856, 88.315073, 176.761855, 263.349105, 354.620147]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import module\n",
    "from tabulate import tabulate\n",
    "  \n",
    "# assign data\n",
    "mydata = [{x[0], y[0]}, \n",
    "          {x[1], y[1]},\n",
    "          {x[2], y[2]},\n",
    "          {x[3], y[3]},\n",
    "          {x[4], y[4]},\n",
    "          {x[5], y[5]},\n",
    "          {x[6], y[6]},\n",
    "          {x[7], y[7]},\n",
    "          {x[8], y[8]},\n",
    "         ]\n",
    "  \n",
    "# create header\n",
    "head = [\"No. lookups\", \"execution time (sec.)\"]\n",
    "  \n",
    "# display table\n",
    "print(tabulate(mydata, headers=head, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(x, y, 'ro', label='XSBench')\n",
    "plt.plot(x1, y1, 'bx', label='RSBench')\n",
    "# plt.title('RSBench')\n",
    "plt.legend()\n",
    "plt.xlabel('The number of lookups')\n",
    "plt.ylabel('Execution time (sec.)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "6*60*60 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ytune",
   "language": "python",
   "name": "ytune"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
