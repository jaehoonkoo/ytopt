{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorial: Autotune the hybrid MPI/OpenMP version of XSBench \n",
    "===================\n",
    "\n",
    "This tutorial describes how to define autotuning problem and an evaluating method for autotuning ECP XSBench app. \n",
    "\n",
    "We assume that you have checked out a copy of `ytopt`. For guidelines on how to get ytopt set up, refer [Install instructions](https://github.com/ytopt-team/ytopt/blob/tutorial/README.md). \n",
    "\n",
    "You can install openmp for this example: `conda install -c conda-forge openmp`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indentifying a problem to autotune \n",
    "-----------------------\n",
    "In this tutorial, we target to autotune ECP XSBench app `<https://github.com/ANL-CESAR/XSBench>`.\n",
    "\n",
    "XSBench is a mini-app representing a key computational kernel of the Monte Carlo neutron transport algorithm [(reference)](https://github.com/ANL-CESAR/XSBench). Save the related source and header files in the seprate folder: `mmp.c`, `Main.c`, `Materials.c`, `XSutils.c`, `XSbench_header.h`, `make.bat`. \n",
    "\n",
    "We omit presenting the files for space. For your convenience, we have the files in `<https://github.com/ytopt-team/ytopt/tree/tutorial/ytopt/benchmark/xsbench-mpi-omp/xsbench>`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining autotuning problem\n",
    "-----------------------\n",
    "We describe how to define your search problem `<https://github.com/ytopt-team/ytopt/blob/tutorial/ytopt/benchmark/xsbench-mpi-omp/xsbench/problem.py>`\n",
    "\n",
    "--------------\n",
    "First, we first define search space using ConfigSpace that is a python library `<https://automl.github.io/ConfigSpace/master/>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required library\n",
    "import os, sys, time, json, math\n",
    "import numpy as np\n",
    "from autotune import TuningProblem\n",
    "from autotune.space import *\n",
    "import ConfigSpace as CS\n",
    "import ConfigSpace.hyperparameters as CSH\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "\n",
    "HERE = os.path.dirname(os.path.abspath(__file__))\n",
    "sys.path.insert(1, os.path.dirname(HERE)+ '/plopper')\n",
    "from plopper import Plopper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our search space contains three parameters: 1) `p0`: number of threads, 2) `p1`: block size for openmp dynamic schedule, 3) `p2`: turn on/off omp parallel.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an object of ConfigSpace \n",
    "cs = CS.ConfigurationSpace(seed=1234)\n",
    "# number of threads\n",
    "p0= CSH.UniformIntegerHyperparameter(name='p0', lower=4, upper=8, default_value=8)\n",
    "#block size for openmp dynamic schedule\n",
    "p1= CSH.OrdinalHyperparameter(name='p1', sequence=['10','20','40','64','80','100','128','160','200'], default_value='100')\n",
    "#omp parallel\n",
    "p2= CSH.CategoricalHyperparameter(name='p2', choices=[\"#pragma omp parallel for\", \" \"], default_value=' ')\n",
    "#add parameters to search space object\n",
    "cs.add_hyperparameters([p0, p1, p2])\n",
    "# problem space\n",
    "input_space = cs\n",
    "output_space = Space([Real(0.0, inf, name=\"time\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "Then, we need to define the objective function `myobj` to evaluate a point in the search space. \n",
    "\n",
    "In this example, we define an evaluating method (Plopper) for code generation and compilation. \n",
    "Plopper take source code and output directory and return an execution time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = os.path.dirname(os.path.realpath(__file__))\n",
    "kernel_idx = dir_path.rfind('/')\n",
    "kernel = dir_path[kernel_idx+1:]\n",
    "obj = Plopper(dir_path+'/mmp.c',dir_path)\n",
    "\n",
    "x1=['p0','p1','p2']\n",
    "def myobj(point: dict):\n",
    "    def plopper_func(x):\n",
    "        x = np.asarray_chkfinite(x)  # ValueError if any NaN or Inf\n",
    "        value = [point[x1[0]],point[x1[1]],point[x1[2]]]\n",
    "        print('CONFIG:',point)\n",
    "        params = [\"P0\",\"P1\",\"P2\"]\n",
    "        result = obj.findRuntime(value, params)\n",
    "        return result\n",
    "    x = np.array([point[f'p{i}'] for i in range(len(point))])\n",
    "    results = plopper_func(x)\n",
    "    print('OUTPUT:%f',results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following describes our evaluating function, Plopper. You can find it `<https://github.com/ytopt-team/ytopt/blob/tutorial/ytopt/benchmark/xsbench-mpi-omp/plopper/plopper.py>`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, subprocess, random\n",
    "\n",
    "class Plopper:\n",
    "    def __init__(self,sourcefile,outputdir):\n",
    "        self.sourcefile = sourcefile\n",
    "        self.outputdir = outputdir+\"/tmp_files\"\n",
    "\n",
    "        if not os.path.exists(self.outputdir):\n",
    "            os.makedirs(self.outputdir)\n",
    "\n",
    "    def createDict(self, x, params):\n",
    "        dictVal = {}\n",
    "        for p, v in zip(params, x):\n",
    "            dictVal[p] = v\n",
    "        return(dictVal)\n",
    "\n",
    "    def plotValues(self, dictVal, inputfile, outputfile):\n",
    "        with open(inputfile, \"r\") as f1:\n",
    "            buf = f1.readlines()\n",
    "\n",
    "        with open(outputfile, \"w\") as f2:\n",
    "            for line in buf:\n",
    "                modify_line = line\n",
    "                for key, value in dictVal.items():\n",
    "                    if key in modify_line:\n",
    "                        if value != 'None': #For empty string options\n",
    "                            modify_line = modify_line.replace('#'+key, str(value))\n",
    "\n",
    "                if modify_line != line:\n",
    "                    f2.write(modify_line)\n",
    "                else:\n",
    "                    f2.write(line)\n",
    "\n",
    "    def findRuntime(self, x, params):\n",
    "        interimfile = \"\"\n",
    "        exetime = 1\n",
    "        counter = random.randint(1, 10001) # To reduce collision increasing the sampling intervals\n",
    "        interimfile = self.outputdir+\"/\"+str(counter)+\".c\"\n",
    "\n",
    "        # Generate intermediate file\n",
    "        dictVal = self.createDict(x, params)\n",
    "        self.plotValues(dictVal, self.sourcefile, interimfile)\n",
    "\n",
    "        #compile and find the execution time\n",
    "        tmpbinary = interimfile[:-2]\n",
    "        kernel_idx = self.sourcefile.rfind('/')\n",
    "        kernel_dir = self.sourcefile[:kernel_idx]\n",
    "\n",
    "        gcc_cmd = \"mpicc -std=gnu99 -Wall -flto  -fopenmp -DOPENMP -DMPI -O3 \"  + \\\n",
    "        \" -o \" + tmpbinary + \" \" + interimfile +\" \" + kernel_dir + \"/Materials.c \" \\\n",
    "        + kernel_dir + \"/XSutils.c \" + \" -I\" + kernel_dir + \\\n",
    "        \" -lm\" + \" -L${CONDA_PREFIX}/lib\"\n",
    "        run_cmd = kernel_dir + \"/exe.pl \" +  tmpbinary\n",
    "\n",
    "        compilation_status = subprocess.run(gcc_cmd, shell=True, stderr=subprocess.PIPE)\n",
    "\n",
    "        #Find the execution time only when the compilation return code is zero, else return infinity\n",
    "        if compilation_status.returncode == 0 :\n",
    "            execution_status = subprocess.run(run_cmd, shell=True, stdout=subprocess.PIPE)\n",
    "            exetime = float(execution_status.stdout.decode('utf-8'))\n",
    "            if exetime == 0:\n",
    "                exetime = 1\n",
    "        else:\n",
    "            print(compilation_status.stderr)\n",
    "            print(\"compile failed\")\n",
    "        return exetime #return execution time as cost "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file consists of several components.\n",
    "\n",
    "`__init__()` takes paths of the source file and output directory, and creates the output directory if it does not exists.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self,sourcefile,outputdir):\n",
    "    # Initilizing global variables\n",
    "    self.sourcefile = sourcefile\n",
    "    self.outputdir = outputdir+\"/tmp_files\"\n",
    "\n",
    "    if not os.path.exists(self.outputdir):\n",
    "        os.makedirs(self.outputdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`createDict()` generates a dictionary for parameter labels and values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDict(self, x, params):\n",
    "    dictVal = {}\n",
    "    for p, v in zip(params, x):\n",
    "        dictVal[p] = v\n",
    "    return(dictVal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`plotValues()` replaces the Markers in the source file with the corresponding prameter values of the parameter dictionary. \n",
    "For example, a sampled value for number of threads `p0` replaces `#P0` in line 349 `input.nthreads = #P0` of `mmp.c` that is the original source file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotValues(self, dictVal, inputfile, outputfile):\n",
    "    with open(inputfile, \"r\") as f1:\n",
    "        buf = f1.readlines()\n",
    "    with open(outputfile, \"w\") as f2:\n",
    "        for line in buf:\n",
    "            modify_line = line\n",
    "            for key, value in dictVal.items():\n",
    "                if key in modify_line:\n",
    "                    if value != 'None': #For empty string options\n",
    "                        modify_line = modify_line.replace('#'+key, str(value))\n",
    "            if modify_line != line:\n",
    "                f2.write(modify_line)\n",
    "            else:\n",
    "                f2.write(line)  #To avoid writing the Marker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`findRuntime()` first calls `createDict()` to obatain configuration values and `plotValues()` to modify the original source code. \n",
    "After that, it generates the commandline `gcc_cmd` for compiling the modified source code and the commandline `run_cmd` for executing the compiled code. \n",
    "Then, it finds the compilation status using subprocess; finds the execution time of the compiled code; and returns the execution time as cost to the search module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findRuntime(self, x, params):\n",
    "    interimfile = \"\"\n",
    "    exetime = 1\n",
    "    counter = random.randint(1, 10001) # To reduce collision increasing the sampling intervals          \n",
    "    interimfile = self.outputdir+\"/tmp_\"+str(counter)+\".c\"\n",
    "\n",
    "    # Generate intermediate file\n",
    "    dictVal = self.createDict(x, params)\n",
    "    self.plotValues(dictVal, self.sourcefile, interimfile)\n",
    "\n",
    "    #compile and find the execution time\n",
    "    tmpbinary = interimfile[:-2]\n",
    "    kernel_idx = self.sourcefile.rfind('/')\n",
    "    kernel_dir = self.sourcefile[:kernel_idx]\n",
    "\n",
    "    gcc_cmd = \"mpicc -std=gnu99 -Wall -flto  -fopenmp -DOPENMP -DMPI -O3 \"  + \\\n",
    "    \" -o \" + tmpbinary + \" \" + interimfile +\" \" + kernel_dir + \"/Materials.c \" \\\n",
    "    + kernel_dir + \"/XSutils.c \" + \" -I\" + kernel_dir + \\\n",
    "    \" -lm\" + \" -L${CONDA_PREFIX}/lib\"\n",
    "    run_cmd = kernel_dir + \"/exe.pl \" +  tmpbinary\n",
    "\n",
    "    #Find the compilation status using subprocess\n",
    "    compilation_status = subprocess.run(gcc_cmd, shell=True, stderr=subprocess.PIPE)\n",
    "\n",
    "    #Find the execution time only when the compilation return code is zero, else return infinity\n",
    "    if compilation_status.returncode == 0 :\n",
    "        execution_status = subprocess.run(run_cmd, shell=True, stdout=subprocess.PIPE)\n",
    "        exetime = float(execution_status.stdout.decode('utf-8'))\n",
    "        if exetime == 0:\n",
    "            exetime = 1\n",
    "    else:\n",
    "        print(compilation_status.stderr)\n",
    "        print(\"compile failed\")\n",
    "    return exetime #return execution time as cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: \n",
    "- `exe.pl` computes average the execution time over multiple runs. We execute once in this example to save time.  \n",
    "\n",
    "--------------\n",
    "Last, we create an object of the autotuning problem. The problem will be called in the commandline implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Problem = TuningProblem(\n",
    "    task_space=None,\n",
    "    input_space=input_space,\n",
    "    output_space=output_space,\n",
    "    objective=myobj,\n",
    "    constraints=None,\n",
    "    model=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running and viewing Results\n",
    "-----------------------\n",
    "\n",
    "Now, we can run the following command to autotune the program: \n",
    "--evaluator flag sets which object used to evaluate models, --problem flag sets path to the Problem instance you want to use for the search, --max-evals flag sets the maximum number of evaluations, --learner flag sets the type of learner (surrogate model).\n",
    "\n",
    "- Go to where `problem.py` such as\n",
    "\n",
    "`\n",
    "cd ytopt/benchmark/xsbench-mpi-omp/xsbench\n",
    "`\n",
    "- Start search\n",
    "\n",
    "`python -m ytopt.search.ambs --evaluator ray --problem problem.Problem --max-evals=10 --learner RF\n",
    "`\n",
    "\n",
    "Note that use `python3` if your environment is built with python3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "Once autotuning kick off, ytopt.log, results.csv, and results.json will be rendered.\n",
    "\n",
    "We can track the results of each run configuration from `ytopt.log` shows the following (output lines are truncated for readability here): "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "2021-08-01 16:40:47|7573|INFO|ytopt.search.search:53] Created \"ray\" evaluator\n",
    "2021-08-01 16:40:47|7573|INFO|ytopt.search.search:54] Evaluator: num_workers is 1\n",
    "2021-08-01 16:40:47|7573|INFO|ytopt.search.hps.ambs:47] Initializing AMBS\n",
    "2021-08-01 16:40:47|7573|INFO|ytopt.search.hps.optimizer.optimizer:51] Using skopt.Optimizer with RF base_estimator\n",
    "2021-08-01 16:40:47|7573|INFO|ytopt.search.hps.ambs:79] Generating 1 initial points...\n",
    "2021-08-01 16:40:47|7573|INFO|ytopt.evaluator.evaluate:104] Submitted new eval of {'p0': '6', 'p1': '200', 'p2': ' '}\n",
    "2021-08-01 16:42:08|7573|INFO|ytopt.evaluator.evaluate:206] New eval finished: {\"p0\": \"6\", \"p1\": \"200\", \"p2\": \" \"} --> 78.081\n",
    "2021-08-01 16:42:08|7573|INFO|ytopt.evaluator.evaluate:217] Requested eval x: {'p0': '6', 'p1': '200', 'p2': ' '} y: 78.081\n",
    "2021-08-01 16:42:08|7573|INFO|ytopt.search.hps.ambs:92] Refitting model with batch of 1 evals\n",
    "2021-08-01 16:42:08|7573|DEBUG|ytopt.search.hps.optimizer.optimizer:119] tell: {'p0': '6', 'p1': '200', 'p2': ' '} --> ('6', '200', ' '): evaluated objective: 78.081\n",
    "2021-08-01 16:42:08|7573|INFO|ytopt.search.hps.ambs:94] Drawing 1 points with strategy cl_max\n",
    "2021-08-01 16:42:08|7573|DEBUG|ytopt.search.hps.optimizer.optimizer:84] _ask: ['5', '64', '#pragma omp parallel for'] lie: 78.081\n",
    "2021-08-01 16:42:08|7573|INFO|ytopt.evaluator.evaluate:104] Submitted new eval of {'p0': '5', 'p1': '64', 'p2': '#pragma omp parallel for'}\n",
    "2021-08-01 16:43:20|7573|INFO|ytopt.evaluator.evaluate:206] New eval finished: {\"p0\": \"5\", \"p1\": \"64\", \"p2\": \"#pragma omp parallel for\"} --> 69.478\n",
    "2021-08-01 16:43:20|7573|INFO|ytopt.evaluator.evaluate:217] Requested eval x: {'p0': '5', 'p1': '64', 'p2': '#pragma omp parallel for'} y: 69.478\n",
    "2021-08-01 16:43:20|7573|INFO|ytopt.search.hps.ambs:92] Refitting model with batch of 1 evals\n",
    "2021-08-01 16:43:20|7573|DEBUG|ytopt.search.hps.optimizer.optimizer:119] tell: {'p0': '5', 'p1': '64', 'p2': '#pragma omp parallel for'} --> ('5', '64', '#pragma omp parallel for'): evaluated objective: 69.478\n",
    "2021-08-01 16:43:20|7573|INFO|ytopt.search.hps.ambs:94] Drawing 1 points with strategy cl_max\n",
    "2021-08-01 16:43:20|7573|DEBUG|ytopt.search.hps.optimizer.optimizer:84] _ask: ['8', '200', '#pragma omp parallel for'] lie: 78.081\n",
    "2021-08-01 16:43:20|7573|INFO|ytopt.evaluator.evaluate:104] Submitted new eval of {'p0': '8', 'p1': '200', 'p2': '#pragma omp parallel for'}\n",
    "2021-08-01 16:44:30|7573|INFO|ytopt.evaluator.evaluate:206] New eval finished: {\"p0\": \"8\", \"p1\": \"200\", \"p2\": \"#pragma omp parallel for\"} --> 68.696\n",
    "2021-08-01 16:44:30|7573|INFO|ytopt.evaluator.evaluate:217] Requested eval x: {'p0': '8', 'p1': '200', 'p2': '#pragma omp parallel for'} y: 68.696\n",
    "2021-08-01 16:44:30|7573|INFO|ytopt.search.hps.ambs:92] Refitting model with batch of 1 evals\n",
    "2021-08-01 16:44:30|7573|DEBUG|ytopt.search.hps.optimizer.optimizer:119] tell: {'p0': '8', 'p1': '200', 'p2': '#pragma omp parallel for'} --> ('8', '200', '#pragma omp parallel for'): evaluated objective: 68.696\n",
    "2021-08-01 16:44:31|7573|INFO|ytopt.search.hps.ambs:94] Drawing 1 points with strategy cl_max\n",
    "2021-08-01 16:44:31|7573|DEBUG|ytopt.search.hps.optimizer.optimizer:84] _ask: ['5', '160', ' '] lie: 78.081\n",
    "2021-08-01 16:44:31|7573|INFO|ytopt.evaluator.evaluate:104] Submitted new eval of {'p0': '5', 'p1': '160', 'p2': ' '}\n",
    "2021-08-01 16:45:42|7573|INFO|ytopt.evaluator.evaluate:206] New eval finished: {\"p0\": \"5\", \"p1\": \"160\", \"p2\": \" \"} --> 69.952\n",
    "2021-08-01 16:45:42|7573|INFO|ytopt.evaluator.evaluate:217] Requested eval x: {'p0': '5', 'p1': '160', 'p2': ' '} y: 69.952\n",
    "2021-08-01 16:45:42|7573|INFO|ytopt.search.hps.ambs:92] Refitting model with batch of 1 evals\n",
    "2021-08-01 16:45:42|7573|DEBUG|ytopt.search.hps.optimizer.optimizer:119] tell: {'p0': '5', 'p1': '160', 'p2': ' '} --> ('5', '160', ' '): evaluated objective: 69.952\n",
    "2021-08-01 16:45:42|7573|INFO|ytopt.search.hps.ambs:94] Drawing 1 points with strategy cl_max\n",
    "2021-08-01 16:45:42|7573|DEBUG|ytopt.search.hps.optimizer.optimizer:84] _ask: ['6', '64', ' '] lie: 78.081\n",
    "2021-08-01 16:45:42|7573|INFO|ytopt.evaluator.evaluate:104] Submitted new eval of {'p0': '6', 'p1': '64', 'p2': ' '}\n",
    "2021-08-01 16:46:46|7573|INFO|ytopt.evaluator.evaluate:206] New eval finished: {\"p0\": \"6\", \"p1\": \"64\", \"p2\": \" \"} --> 62.711\n",
    "2021-08-01 16:46:46|7573|INFO|ytopt.evaluator.evaluate:217] Requested eval x: {'p0': '6', 'p1': '64', 'p2': ' '} y: 62.711\n",
    "2021-08-01 16:46:46|7573|INFO|ytopt.search.hps.ambs:92] Refitting model with batch of 1 evals\n",
    "2021-08-01 16:46:46|7573|DEBUG|ytopt.search.hps.optimizer.optimizer:119] tell: {'p0': '6', 'p1': '64', 'p2': ' '} --> ('6', '64', ' '): evaluated objective: 62.711\n",
    "2021-08-01 16:46:47|7573|INFO|ytopt.search.hps.ambs:94] Drawing 1 points with strategy cl_max\n",
    "2021-08-01 16:46:47|7573|DEBUG|ytopt.search.hps.optimizer.optimizer:84] _ask: ['8', '64', '#pragma omp parallel for'] lie: 78.081\n",
    "2021-08-01 16:46:47|7573|INFO|ytopt.evaluator.evaluate:104] Submitted new eval of {'p0': '8', 'p1': '64', 'p2': '#pragma omp parallel for'}\n",
    "2021-08-01 16:47:56|7573|INFO|ytopt.evaluator.evaluate:206] New eval finished: {\"p0\": \"8\", \"p1\": \"64\", \"p2\": \"#pragma omp parallel for\"} --> 68.116\n",
    "2021-08-01 16:47:56|7573|INFO|ytopt.evaluator.evaluate:217] Requested eval x: {'p0': '8', 'p1': '64', 'p2': '#pragma omp parallel for'} y: 68.116\n",
    "2021-08-01 16:47:56|7573|INFO|ytopt.search.hps.ambs:92] Refitting model with batch of 1 evals\n",
    "2021-08-01 16:47:56|7573|DEBUG|ytopt.search.hps.optimizer.optimizer:119] tell: {'p0': '8', 'p1': '64', 'p2': '#pragma omp parallel for'} --> ('8', '64', '#pragma omp parallel for'): evaluated objective: 68.116\n",
    "2021-08-01 16:47:57|7573|INFO|ytopt.search.hps.ambs:94] Drawing 1 points with strategy cl_max\n",
    "2021-08-01 16:47:57|7573|DEBUG|ytopt.search.hps.optimizer.optimizer:84] _ask: ['8', '64', ' '] lie: 78.081\n",
    "2021-08-01 16:47:57|7573|INFO|ytopt.evaluator.evaluate:104] Submitted new eval of {'p0': '8', 'p1': '64', 'p2': ' '}\n",
    "2021-08-01 16:49:06|7573|INFO|ytopt.evaluator.evaluate:206] New eval finished: {\"p0\": \"8\", \"p1\": \"64\", \"p2\": \" \"} --> 67.574\n",
    "2021-08-01 16:49:06|7573|INFO|ytopt.evaluator.evaluate:217] Requested eval x: {'p0': '8', 'p1': '64', 'p2': ' '} y: 67.574\n",
    "2021-08-01 16:49:06|7573|INFO|ytopt.search.hps.ambs:92] Refitting model with batch of 1 evals\n",
    "2021-08-01 16:49:06|7573|DEBUG|ytopt.search.hps.optimizer.optimizer:119] tell: {'p0': '8', 'p1': '64', 'p2': ' '} --> ('8', '64', ' '): evaluated objective: 67.574\n",
    "2021-08-01 16:49:06|7573|INFO|ytopt.search.hps.ambs:94] Drawing 1 points with strategy cl_max\n",
    "2021-08-01 16:49:06|7573|DEBUG|ytopt.search.hps.optimizer.optimizer:84] _ask: ['7', '64', ' '] lie: 78.081\n",
    "2021-08-01 16:49:06|7573|INFO|ytopt.evaluator.evaluate:104] Submitted new eval of {'p0': '7', 'p1': '64', 'p2': ' '}\n",
    "2021-08-01 16:50:22|7573|INFO|ytopt.evaluator.evaluate:206] New eval finished: {\"p0\": \"7\", \"p1\": \"64\", \"p2\": \" \"} --> 73.099\n",
    "2021-08-01 16:50:22|7573|INFO|ytopt.evaluator.evaluate:217] Requested eval x: {'p0': '7', 'p1': '64', 'p2': ' '} y: 73.099\n",
    "2021-08-01 16:50:22|7573|INFO|ytopt.search.hps.ambs:92] Refitting model with batch of 1 evals\n",
    "2021-08-01 16:50:22|7573|DEBUG|ytopt.search.hps.optimizer.optimizer:119] tell: {'p0': '7', 'p1': '64', 'p2': ' '} --> ('7', '64', ' '): evaluated objective: 73.099\n",
    "2021-08-01 16:50:22|7573|INFO|ytopt.search.hps.ambs:94] Drawing 1 points with strategy cl_max\n",
    "2021-08-01 16:50:22|7573|DEBUG|ytopt.search.hps.optimizer.optimizer:84] _ask: ['4', '64', ' '] lie: 78.081\n",
    "2021-08-01 16:50:22|7573|INFO|ytopt.evaluator.evaluate:104] Submitted new eval of {'p0': '4', 'p1': '64', 'p2': ' '}\n",
    "2021-08-01 16:51:48|7573|INFO|ytopt.evaluator.evaluate:206] New eval finished: {\"p0\": \"4\", \"p1\": \"64\", \"p2\": \" \"} --> 83.507\n",
    "2021-08-01 16:51:48|7573|INFO|ytopt.evaluator.evaluate:217] Requested eval x: {'p0': '4', 'p1': '64', 'p2': ' '} y: 83.507\n",
    "2021-08-01 16:51:48|7573|INFO|ytopt.search.hps.ambs:92] Refitting model with batch of 1 evals\n",
    "2021-08-01 16:51:48|7573|DEBUG|ytopt.search.hps.optimizer.optimizer:119] tell: {'p0': '4', 'p1': '64', 'p2': ' '} --> ('4', '64', ' '): evaluated objective: 83.507\n",
    "2021-08-01 16:51:48|7573|INFO|ytopt.search.hps.ambs:94] Drawing 1 points with strategy cl_max\n",
    "2021-08-01 16:51:48|7573|DEBUG|ytopt.search.hps.optimizer.optimizer:84] _ask: ['5', '64', ' '] lie: 83.507\n",
    "2021-08-01 16:51:49|7573|INFO|ytopt.evaluator.evaluate:104] Submitted new eval of {'p0': '5', 'p1': '64', 'p2': ' '}\n",
    "2021-08-01 16:53:06|7573|INFO|ytopt.evaluator.evaluate:206] New eval finished: {\"p0\": \"5\", \"p1\": \"64\", \"p2\": \" \"} --> 74.946\n",
    "2021-08-01 16:53:06|7573|INFO|ytopt.evaluator.evaluate:217] Requested eval x: {'p0': '5', 'p1': '64', 'p2': ' '} y: 74.946\n",
    "2021-08-01 16:53:06|7573|INFO|ytopt.search.hps.ambs:101] Hyperopt driver finishing\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look up the best configuration (found so far) and its value by inspecting the following created file: `results.csv` and `results.json`. \n",
    "\n",
    "In this run, the best configuration and its runtime is obtained:\n",
    "\n",
    "`{'p0': '6', 'p1': '64', 'p2': ' '}: 62.711`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
